{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mWhjnZNVC_u"
      },
      "source": [
        "## Creating ChatBot Using Natural Language Processing in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5sy_-IiVC_w"
      },
      "source": [
        "### What is a chatbot ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcMk1G-VC_x"
      },
      "source": [
        "##### A chatbot is a computer program that is designed to simulate conversation with human users, typically over messaging platforms, such as Facebook Messenger, WhatsApp, or Slack. Chatbots use Natural Language Processing (NLP) and Artificial Intelligence (AI) to understand and respond to user queries in a human-like manner.\n",
        "\n",
        "##### Chatbots can be designed to perform a variety of tasks, such as answering customer queries, providing product recommendations, scheduling appointments, or even just engaging in casual conversation. Some chatbots are rule-based, meaning they respond based on pre-programmed rules, while others are more advanced and use machine learning algorithms to learn from user interactions and improve their responses over time.\n",
        "\n",
        "##### Chatbots have become increasingly popular in recent years, as they provide a cost-effective and scalable way for businesses to provide customer support and interact with users. They can be accessed 24/7, and can handle a high volume of inquiries without the need for human intervention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJFKUHdQVC_y"
      },
      "source": [
        "Here are some of the major fields of Natural Language Processing (NLP):\n",
        "\n",
        "- Morphology: This field deals with the study of the internal structure of words and how they are formed.\n",
        "\n",
        "- Syntax: This field deals with the study of the structure of sentences and the rules governing the arrangement of words and phrases.\n",
        "\n",
        "- Semantics: This field deals with the study of meaning in language and how words and sentences are used to convey meaning.\n",
        "\n",
        "- Discourse Analysis: This field deals with the study of how language is used in a given context, including the relationship between speakers, the intended audience, and the purpose of communication.\n",
        "\n",
        "- Text Mining: This field involves the use of statistical and machine learning techniques to analyze and extract information from large collections of text.\n",
        "\n",
        "- Sentiment Analysis: This field involves the use of NLP techniques to automatically determine the sentiment or emotion expressed in a piece of text.\n",
        "\n",
        "- Named Entity Recognition: This field involves the use of NLP techniques to automatically identify and classify named entities such as people, organizations, and locations in text.\n",
        "\n",
        "- Machine Translation: This field involves the use of NLP techniques to automatically translate text from one language to another.\n",
        "\n",
        "- Speech Recognition: This field involves the use of NLP techniques to automatically transcribe spoken language into text.\n",
        "\n",
        "- Text-to-Speech: This field involves the use of NLP techniques to convert text into spoken language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147QCeYoVC_z"
      },
      "source": [
        "###### Note : \n",
        "\n",
        "- In Natural Language Processing (NLP), we often work with JSON data because JSON is a lightweight and flexible data format that is easy to read and write for both humans and machines. JSON stands for \"JavaScript Object Notation\" and it is a text-based format for representing data in the form of key-value pairs.\n",
        "\n",
        "- JSON is widely used in web development and API (Application Programming Interface) design, and many NLP tools and platforms also use JSON to represent and exchange data. For example, in NLP, we may use JSON to represent text data, such as sentences or documents, along with metadata such as author, date, or source.\n",
        "\n",
        "- JSON data can be easily parsed and manipulated by programming languages such as Python, making it a popular choice for NLP tasks that involve data preprocessing, analysis, or modeling. JSON also allows for hierarchical and nested data structures, which can be useful for representing complex linguistic data such as parse trees or dependency graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhPEzJl7VC_0"
      },
      "source": [
        "### 1 - Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o24NOOiaVC_0"
      },
      "source": [
        "- JSON: It is possible to utilize it to work with JSON data.\n",
        "\n",
        "- String: Provides access to several potentially valuable constants.\n",
        "\n",
        "- Random: For various distributions, this module implements pseudo-random number generators.\n",
        "\n",
        "- WordNetLemmatizer: It can lemmatize. In other terms, Lemmatization is the process of reducing a word to its base or root form. WordNetLemmatizer is available through the NLTK (Natural Language Toolkit) library\n",
        "\n",
        "- Tensorflow: A multidimensional array of elements is represented by this symbol.\n",
        "\n",
        "- Sequential: Sequential groups a linear stack of layers into a tf.keras.Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIVBFxNSVC_1",
        "outputId": "ddfd6646-e157-4461-910e-b5c37210f5a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "import numpy as num\n",
        "from nltk.stem import WordNetLemmatizer # It has the ability to lemmatize.\n",
        "import tensorflow as tensorF # A multidimensional array of elements is represented by this symbol.\n",
        "from tensorflow.keras import Sequential # Sequential groups a linear stack of layers into a tf.keras.Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "nltk.download(\"punkt\")# required package for tokenization\n",
        "nltk.download(\"wordnet\")# word database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9bmoKh2VC_2"
      },
      "source": [
        "##### Tokenization is the process of breaking down a text into smaller units, called tokens. In Natural Language Processing (NLP), tokenization is often the first step in processing text data. The resulting tokens can then be used for various NLP tasks such as text classification, sentiment analysis, and machine translation.\n",
        "\n",
        "##### A token can be defined as a sequence of characters that represents a unit of meaning. The most common type of tokenization involves breaking down text into words, also known as word tokenization. However, other types of tokenization can involve breaking down text into individual characters, phrases, or sentences.\n",
        "\n",
        "##### In word tokenization, the input text is typically split into words based on whitespace or punctuation marks. For example, the sentence \"I love natural language processing!\" could be tokenized into the following words: \"I\", \"love\", \"natural\", \"language\", \"processing\". Some tokenization algorithms may also take into account additional factors such as capitalization and context.\n",
        "\n",
        "##### Tokenization can be performed using a variety of tools and libraries in NLP, such as NLTK (Natural Language Toolkit) and spaCy in Python. These tools provide pre-trained models and functions for performing tokenization, as well as options for customizing the tokenization process based on specific needs.\n",
        "\n",
        "##### Tokenization is a fundamental step in many NLP tasks, as it enables the computer to process and analyze text data at a more granular level. By breaking down text into smaller units, tokenization helps to reduce the complexity of the data and makes it easier for NLP algorithms to extract meaningful information from the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf62mmEHVC_3",
        "outputId": "33446524-210d-4ea3-a0c8-a6b00c13aca1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ourIntents': [{'tag': 'age',\n",
              "   'patterns': ['how old are you?'],\n",
              "   'responses': ['I am 2 years old and my birthday was yesterday']},\n",
              "  {'tag': 'greeting',\n",
              "   'patterns': ['Hi', 'Hello', 'Hey'],\n",
              "   'responses': ['Hi there', 'Hello', 'Hi :)']},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['bye', 'later'],\n",
              "   'responses': ['Bye', 'take care']},\n",
              "  {'tag': 'name',\n",
              "   'patterns': [\"what's your name?\", 'who are you?'],\n",
              "   'responses': ['I have no name yet',\n",
              "    'You can give me one, and I will appreciate it']},\n",
              "  {'tag': 'exit',\n",
              "   'patterns': ['quit', 'exit'],\n",
              "   'responses': ['Okay, Good bye!!', 'Bye, Have a nice day ']}]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#3 Loading the Dataset: intents.json\n",
        "\n",
        "data_file = open('/content/Data.json').read()\n",
        "data = json.loads(data_file)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyac9yRqVC_3"
      },
      "source": [
        "### Processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZKDe_V9iVC_4"
      },
      "outputs": [],
      "source": [
        "lm = WordNetLemmatizer() #reducing words to their base or dictionary form\n",
        "\n",
        "ourClasses = []\n",
        "newWords = []\n",
        "documentX = []\n",
        "documentY = []\n",
        "# Each intent is tokenized into words and the patterns and their associated tags are added to their respective lists.\n",
        "for intent in data[\"ourIntents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        ournewTkns = nltk.word_tokenize(pattern)\n",
        "        newWords.extend(ournewTkns)\n",
        "        documentX.append(pattern)\n",
        "        documentY.append(intent['tag'])\n",
        "    if intent[\"tag\"] not in ourClasses:\n",
        "        ourClasses.append(intent[\"tag\"])\n",
        "\n",
        "newWords = [lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation]\n",
        "newWords = sorted(set(newWords))\n",
        "ourClasses = sorted(set(ourClasses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gd5bWaEVC_4"
      },
      "source": [
        "This code seems to be preparing the data that will be used to train an NLP model to recognize intents and generate appropriate responses. The newWords list is likely to be used to create a vocabulary of all the unique words that appear in the training data, while the documentX and documentY lists are probably going to be used as the input and output data for the NLP model. The ourClasses list may be used to define the set of possible intents that the chatbot can understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcaOLO-fVC_5"
      },
      "source": [
        "### Designing a neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ70cQNHVC_5"
      },
      "source": [
        "The code below is used to turn our data into numerical values using bag of words (BoW) encoding system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zhmIOSlEVC_6"
      },
      "outputs": [],
      "source": [
        "trainingData  = []\n",
        "outEmpty = [0] * len(ourClasses)\n",
        "\n",
        "for idx, doc in enumerate(documentX):\n",
        "    bag0words = []\n",
        "    text = lm.lemmatize(doc.lower())\n",
        "    for word in newWords :\n",
        "        bag0words.append(1) if word in text else bag0words.append(0)\n",
        "\n",
        "    outputRow = list(outEmpty)\n",
        "    outputRow[ourClasses.index(documentY[idx])] = 1 \n",
        "    trainingData.append([bag0words, outputRow])\n",
        "\n",
        "random.shuffle(trainingData)\n",
        "trainingData = num.array(trainingData, dtype=object)\n",
        "\n",
        "x = num.array(list(trainingData[:,0]))\n",
        "y = num.array(list(trainingData[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlrsadf4VC_6"
      },
      "source": [
        "- DocumentX contains a list of documents (or text data) to be classified.\n",
        "\n",
        "- lm.lemmatize is a method that is likely used to perform lemmatization on the text data. \n",
        "\n",
        "- Lemmatization is the process of reducing a word to its base form (e.g., \"running\" to \"run\").\n",
        "\n",
        "- newWords appears to be a list of words that the model will use to create the bag-of-words representation of the text data.\n",
        "\n",
        "- For each document, the code creates a bag0words list that contains 1's and 0's to indicate whether each word in newWords appears in the document. This is the bag-of-words representation.\n",
        "\n",
        "- ourClasses appears to be a list of the classes (or labels) that the model will be trained to predict.\n",
        "\n",
        "- documentY contains a list of labels corresponding to each document in documentX.\n",
        "\n",
        "- outEmpty is a list of 0's with length equal to the number of classes.\n",
        "\n",
        "- For each document, the code creates an outputRow list that is initialized with outEmpty, and then sets the corresponding index to 1 to indicate the \n",
        "correct class for that document.\n",
        "\n",
        "The trainingData list is created by appending each bag0words list and outputRow list as a pair.\n",
        "\n",
        "- The trainingData list is shuffled to randomize the order of the pairs.\n",
        "\n",
        "- num.array is used to create arrays from the bag0words and outputRow lists in trainingData, which are assigned to x and y, respectively.\n",
        "\n",
        "- Overall, this code appears to be setting up the data for a text classification model that uses a bag-of-words representation and a supervised learning approach. However, it's difficult to say exactly how this data will be used without seeing the rest of the code that trains and evaluates the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l170EGLDVC_7"
      },
      "source": [
        "#### Defining and training a neural network model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKr0aF3PVC_7",
        "outputId": "82991083-48e7-49d0-fe8b-de1f71667025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 128)               2176      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,757\n",
            "Trainable params: 10,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6601 - accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5604 - accuracy: 0.3000\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4836 - accuracy: 0.8000\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3738 - accuracy: 0.6000\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3533 - accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1544 - accuracy: 0.9000\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9386 - accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0851 - accuracy: 0.9000\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8096 - accuracy: 0.8000\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6612 - accuracy: 0.9000\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7081 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6018 - accuracy: 0.9000\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4927 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4504 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1765 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2522 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3769 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1562 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0678 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0767 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0598 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1536 - accuracy: 0.9000\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0599 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0700 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.8328e-04 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.3944e-04 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.8694e-04 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.5849e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.7443e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.3038e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.4150e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7370e-05 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.4225e-05 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.8479e-05 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0989e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.1161e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.8873e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0244 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.6931e-04 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6855e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.2195e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7214e-05 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2269e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5299e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.4522e-05 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1623e-05 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.5362e-05 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0163 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7440e-05 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3065e-04 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.3038e-05 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.2751e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.4123e-05 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9766e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2340e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5506e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1456 - accuracy: 0.9000\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.4501e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8644e-05 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9010e-04 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.8617e-05 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0462 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6433e-04 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6308e-05 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.3328e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0705e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9261e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7058e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3494e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3917e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.4175e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9966e-04 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.3804e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.9146e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.7099e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.4887e-04 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5057e-04 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.3790e-04 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7185e-04 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4532 - accuracy: 0.9000\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.6041e-06 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9216e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9833e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.4962e-04 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.6807e-04 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.6854e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5320e-04 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.1744e-04 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.7319e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.1478e-04 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.2834e-04 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.0387e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1285e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.3915e-04 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2278e-04 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 4.7606e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1665e-04 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.4602e-04 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7429e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4413e-04 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.8780e-04 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5178e-04 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.6303e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1539e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.4492e-06 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1740e-04 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.9489e-04 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6531e-06 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.1661e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7123e-04 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0943e-04 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6437e-04 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.9295e-04 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6689e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.9120e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8507e-04 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9723e-06 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.0125e-04 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.1736e-06 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.7166e-06 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.9417e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.3789e-06 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2219e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8465e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6523e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5752e-04 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.1588e-04 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4699e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1217e-04 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.6862e-06 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.2094e-06 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6458e-04 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1358e-04 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.3146e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3666e-04 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1408e-04 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4957e-04 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.1337e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1127e-06 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.7961e-06 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0396e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.0425e-06 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2327e-04 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 7.4018e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.1214e-05 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2517e-06 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2743e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.7005e-04 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9990e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2598e-04 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.2242e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.3645e-04 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7997e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9655e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f665aff17c0>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iShape = (len(x[0]),)\n",
        "oShape = len(y[0])\n",
        "\n",
        "Model = Sequential()\n",
        "\n",
        "Model.add(Dense(128, activation=\"relu\" , input_shape=iShape))\n",
        "\n",
        "Model.add(Dropout(0.5)) #Dropout is a regularization technique that randomly drops out (sets to zero) some of the inputs to a layer during training to prevent overfitting.\n",
        "\n",
        "Model.add(Dense(64, activation=\"relu\"))\n",
        "\n",
        "Model.add(Dropout(0.3))\n",
        "\n",
        "Model.add(Dense(oShape, activation='softmax'))\n",
        "\n",
        "md = tensorF.keras.optimizers.Adam(learning_rate= 0.01)\n",
        "\n",
        "Model.compile(optimizer=md, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(Model.summary())\n",
        "\n",
        "Model.fit(x,y, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVYofNS0VC_8"
      },
      "source": [
        "#### Building useful features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YsSEa3LAxY8z"
      },
      "outputs": [],
      "source": [
        "def ourText(text):\n",
        "  newtkns = nltk.word_tokenize(text)\n",
        "  newtkns = [lm.lemmatize(word) for word in newtkns]\n",
        "  return newtkns\n",
        "\n",
        "def wordBag(text, vocab):\n",
        "  newtkns = ourText(text)\n",
        "  bagOwords = [0] * len(vocab)\n",
        "  for w in newtkns:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bagOwords[idx] = 1\n",
        "  return num.array(bagOwords)\n",
        "\n",
        "def Pclass(text, vocab, labels):\n",
        "  bagOwords = wordBag(text, vocab)\n",
        "  ourResult = Model.predict(num.array([bagOwords]))[0]\n",
        "  newThresh = 0.2\n",
        "  yp = [[idx, res] for idx, res in enumerate(ourResult) if res > newThresh]\n",
        "\n",
        "  yp.sort(key=lambda x: x[1], reverse=True)\n",
        "  newList = []\n",
        "  for r in yp:\n",
        "    newList.append(labels[r[0]])\n",
        "  return newList\n",
        "\n",
        "def getRes(firstlist, fJson):\n",
        "  tag = firstlist[0]\n",
        "  listOfIntents = fJson[\"ourIntents\"]\n",
        "  for i in listOfIntents:\n",
        "    if i[\"tag\"] == tag:\n",
        "      ourResult = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  return ourResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNnn2QdoxjU6",
        "outputId": "368ef015-a851-41da-f7df-bf9d5c0017d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 65ms/step\n",
            "Hi there\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Hi there\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Okay, Good bye!!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    newMessage = input(\"\")\n",
        "    intents = Pclass(newMessage, newWords, ourClasses)\n",
        "    ourResult = getRes(intents, data)\n",
        "    print(ourResult)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## And yeah!! This is our little chatbot *_*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Just to mention in the end of our notebook that this is a simple chatbot which is a rule based chatbot, that can answer just questions existing in the JSON file, and if we ask a new unexisting question this chatbot will answer with anaccurate answers, in contarary with AI based chatbots which learn from data and adapt its responses over time. AI chatbots are trained on large datasets of conversations, which helps them to understand the nuances of language and provide more personalized responses. They can handle a wider range of inputs and can provide more intelligent and sophisticated responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
