{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mWhjnZNVC_u"
      },
      "source": [
        "## Creating ChatBot Using Natural Language Processing in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5sy_-IiVC_w"
      },
      "source": [
        "### What is a chatbot ? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcMk1G-VC_x"
      },
      "source": [
        "##### A chatbot is a computer program that is designed to simulate conversation with human users, typically over messaging platforms, such as Facebook Messenger, WhatsApp, or Slack. Chatbots use Natural Language Processing (NLP) and Artificial Intelligence (AI) to understand and respond to user queries in a human-like manner.\n",
        "\n",
        "##### Chatbots can be designed to perform a variety of tasks, such as answering customer queries, providing product recommendations, scheduling appointments, or even just engaging in casual conversation. Some chatbots are rule-based, meaning they respond based on pre-programmed rules, while others are more advanced and use machine learning algorithms to learn from user interactions and improve their responses over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJFKUHdQVC_y"
      },
      "source": [
        "Here are some of the major fields of Natural Language Processing (NLP):\n",
        "\n",
        "- Morphology: This field deals with the study of the internal structure of words and how they are formed.\n",
        "\n",
        "- Syntax: This field deals with the study of the structure of sentences and the rules governing the arrangement of words and phrases.\n",
        "\n",
        "- Semantics: This field deals with the study of meaning in language and how words and sentences are used to convey meaning.\n",
        "\n",
        "- Discourse Analysis: This field deals with the study of how language is used in a given context, including the relationship between speakers, the intended audience, and the purpose of communication.\n",
        "\n",
        "- Text Mining: This field involves the use of statistical and machine learning techniques to analyze and extract information from large collections of text.\n",
        "\n",
        "- Sentiment Analysis: This field involves the use of NLP techniques to automatically determine the sentiment or emotion expressed in a piece of text.\n",
        "\n",
        "- Named Entity Recognition: This field involves the use of NLP techniques to automatically identify and classify named entities such as people, organizations, and locations in text.\n",
        "\n",
        "- Machine Translation: This field involves the use of NLP techniques to automatically translate text from one language to another.\n",
        "\n",
        "- Speech Recognition: This field involves the use of NLP techniques to automatically transcribe spoken language into text.\n",
        "\n",
        "- Text-to-Speech: This field involves the use of NLP techniques to convert text into spoken language."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "147QCeYoVC_z"
      },
      "source": [
        "#### Note : \n",
        "\n",
        "- In Natural Language Processing (NLP), we often work with JSON data because JSON is a lightweight and flexible data format that is easy to read and write for both humans and machines. JSON stands for \"JavaScript Object Notation\" and it is a text-based format for representing data in the form of key-value pairs.\n",
        "\n",
        "- JSON is widely used in web development and API (Application Programming Interface) design, and many NLP tools and platforms also use JSON to represent and exchange data. For example, in NLP, we may use JSON to represent text data, such as sentences or documents, along with metadata such as author, date, or source.\n",
        "\n",
        "- JSON data can be easily parsed and manipulated by programming languages such as Python, making it a popular choice for NLP tasks that involve data preprocessing, analysis, or modeling. JSON also allows for hierarchical and nested data structures, which can be useful for representing complex linguistic data such as parse trees or dependency graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhPEzJl7VC_0"
      },
      "source": [
        "### 1 - Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o24NOOiaVC_0"
      },
      "source": [
        "- JSON: It is possible to utilize it to work with JSON data.\n",
        "\n",
        "- String: Provides access to several potentially valuable constants.\n",
        "\n",
        "- Random: For various distributions, this module implements pseudo-random number generators.\n",
        "\n",
        "- WordNetLemmatizer: It can lemmatize. In other terms, Lemmatization is the process of reducing a word to its base or root form. WordNetLemmatizer is available through the NLTK (Natural Language Toolkit) library\n",
        "\n",
        "- Tensorflow: A multidimensional array of elements is represented by this symbol.\n",
        "\n",
        "- Sequential: Sequential groups a linear stack of layers into a tf.keras.Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIVBFxNSVC_1",
        "outputId": "ddfd6646-e157-4461-910e-b5c37210f5a6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "import numpy as num\n",
        "from nltk.stem import WordNetLemmatizer # It has the ability to lemmatize.\n",
        "import tensorflow as tensorF # A multidimensional array of elements is represented by this symbol.\n",
        "from tensorflow.keras import Sequential # Sequential groups a linear stack of layers into a tf.keras.Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "nltk.download(\"punkt\")# required package for tokenization\n",
        "nltk.download(\"wordnet\")# word database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf62mmEHVC_3",
        "outputId": "33446524-210d-4ea3-a0c8-a6b00c13aca1"
      },
      "outputs": [],
      "source": [
        "#3 Loading the Dataset: intents.json\n",
        "\n",
        "data_file = open('/content/Data.json').read()\n",
        "data = json.loads(data_file)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyac9yRqVC_3"
      },
      "source": [
        "### Processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKDe_V9iVC_4"
      },
      "outputs": [],
      "source": [
        "lm = WordNetLemmatizer() #reducing words to their base or dictionary form\n",
        "\n",
        "ourClasses = []\n",
        "newWords = []\n",
        "documentX = []\n",
        "documentY = []\n",
        "# Each intent is tokenized into words and the patterns and their associated tags are added to their respective lists.\n",
        "for intent in data[\"ourIntents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        ournewTkns = nltk.word_tokenize(pattern)\n",
        "        newWords.extend(ournewTkns)\n",
        "        documentX.append(pattern)\n",
        "        documentY.append(intent['tag'])\n",
        "    if intent[\"tag\"] not in ourClasses:\n",
        "        ourClasses.append(intent[\"tag\"])\n",
        "\n",
        "newWords = [lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation]\n",
        "newWords = sorted(set(newWords))\n",
        "ourClasses = sorted(set(ourClasses))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gd5bWaEVC_4"
      },
      "source": [
        "This is the preparation of the data that will be used to train an NLP model to recognize intents and generate appropriate responses. The newWords list is likely to be used to create a vocabulary of all the unique words that appear in the training data, while the documentX and documentY lists are probably going to be used as the input and output data for the NLP model. The ourClasses list may be used to define the set of possible intents that the chatbot can understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcaOLO-fVC_5"
      },
      "source": [
        "### Designing a neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ70cQNHVC_5"
      },
      "source": [
        "The code below is used to turn our data into numerical values using bag of words (BoW) encoding system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhmIOSlEVC_6"
      },
      "outputs": [],
      "source": [
        "trainingData  = []\n",
        "outEmpty = [0] * len(ourClasses)\n",
        "\n",
        "for idx, doc in enumerate(documentX):\n",
        "    bag0words = []\n",
        "    text = lm.lemmatize(doc.lower())\n",
        "    for word in newWords :\n",
        "        bag0words.append(1) if word in text else bag0words.append(0)\n",
        "\n",
        "    outputRow = list(outEmpty)\n",
        "    outputRow[ourClasses.index(documentY[idx])] = 1 \n",
        "    trainingData.append([bag0words, outputRow])\n",
        "\n",
        "random.shuffle(trainingData)\n",
        "trainingData = num.array(trainingData, dtype=object)\n",
        "\n",
        "x = num.array(list(trainingData[:,0]))\n",
        "y = num.array(list(trainingData[:,1]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlrsadf4VC_6"
      },
      "source": [
        "- DocumentX contains a list of documents (or text data) to be classified.\n",
        "\n",
        "- lm.lemmatize is a method that is likely used to perform lemmatization on the text data. \n",
        "\n",
        "- Lemmatization is the process of reducing a word to its base form (e.g., \"running\" to \"run\").\n",
        "\n",
        "- newWords appears to be a list of words that the model will use to create the bag-of-words representation of the text data.\n",
        "\n",
        "- For each document, the code creates a bag0words list that contains 1's and 0's to indicate whether each word in newWords appears in the document. This is the bag-of-words representation.\n",
        "\n",
        "- ourClasses appears to be a list of the classes (or labels) that the model will be trained to predict.\n",
        "\n",
        "- documentY contains a list of labels corresponding to each document in documentX.\n",
        "\n",
        "- outEmpty is a list of 0's with length equal to the number of classes.\n",
        "\n",
        "- For each document, the code creates an outputRow list that is initialized with outEmpty, and then sets the corresponding index to 1 to indicate the \n",
        "correct class for that document.\n",
        "\n",
        "The trainingData list is created by appending each bag0words list and outputRow list as a pair.\n",
        "\n",
        "- The trainingData list is shuffled to randomize the order of the pairs.\n",
        "\n",
        "- num.array is used to create arrays from the bag0words and outputRow lists in trainingData, which are assigned to x and y, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l170EGLDVC_7"
      },
      "source": [
        "#### Defining and training a neural network model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKr0aF3PVC_7",
        "outputId": "82991083-48e7-49d0-fe8b-de1f71667025"
      },
      "outputs": [],
      "source": [
        "iShape = (len(x[0]),)\n",
        "oShape = len(y[0])\n",
        "\n",
        "Model = Sequential()\n",
        "\n",
        "Model.add(Dense(128, activation=\"relu\" , input_shape=iShape))\n",
        "\n",
        "Model.add(Dropout(0.5)) #Dropout is a regularization technique that randomly drops out (sets to zero) some of the inputs to a layer during training to prevent overfitting.\n",
        "\n",
        "Model.add(Dense(64, activation=\"relu\"))\n",
        "\n",
        "Model.add(Dropout(0.3))\n",
        "\n",
        "Model.add(Dense(oShape, activation='softmax'))\n",
        "\n",
        "md = tensorF.keras.optimizers.Adam(learning_rate= 0.01)\n",
        "\n",
        "Model.compile(optimizer=md, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(Model.summary())\n",
        "\n",
        "Model.fit(x,y, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVYofNS0VC_8"
      },
      "source": [
        "#### Building useful features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSEa3LAxY8z"
      },
      "outputs": [],
      "source": [
        "def ourText(text):\n",
        "  newtkns = nltk.word_tokenize(text)\n",
        "  newtkns = [lm.lemmatize(word) for word in newtkns]\n",
        "  return newtkns\n",
        "\n",
        "def wordBag(text, vocab):\n",
        "  newtkns = ourText(text)\n",
        "  bagOwords = [0] * len(vocab)\n",
        "  for w in newtkns:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bagOwords[idx] = 1\n",
        "  return num.array(bagOwords)\n",
        "\n",
        "def Pclass(text, vocab, labels):\n",
        "  bagOwords = wordBag(text, vocab)\n",
        "  ourResult = Model.predict(num.array([bagOwords]))[0]\n",
        "  newThresh = 0.2\n",
        "  yp = [[idx, res] for idx, res in enumerate(ourResult) if res > newThresh]\n",
        "\n",
        "  yp.sort(key=lambda x: x[1], reverse=True)\n",
        "  newList = []\n",
        "  for r in yp:\n",
        "    newList.append(labels[r[0]])\n",
        "  return newList\n",
        "\n",
        "def getRes(firstlist, fJson):\n",
        "  tag = firstlist[0]\n",
        "  listOfIntents = fJson[\"ourIntents\"]\n",
        "  for i in listOfIntents:\n",
        "    if i[\"tag\"] == tag:\n",
        "      ourResult = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  return ourResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNnn2QdoxjU6",
        "outputId": "368ef015-a851-41da-f7df-bf9d5c0017d1"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    newMessage = input(\"\")\n",
        "    intents = Pclass(newMessage, newWords, ourClasses)\n",
        "    ourResult = getRes(intents, data)\n",
        "    print(ourResult)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## And yeah!! This is our little chatbot *_*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### At the end of our notebook, I want to mention that this is a simple chatbot which is a rule based chatbot, that can answer just questions existing in the JSON file, and if we ask a new unexisting question this chatbot will answer with anaccurate answers, in contarary with AI based chatbots which learn from data and adapt its responses over time. AI chatbots are trained on large datasets of conversations, which helps them to understand the nuances of language and provide more personalized responses. They can handle a wider range of inputs and can provide more intelligent and sophisticated responses"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
