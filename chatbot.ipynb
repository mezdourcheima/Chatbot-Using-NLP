{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ChatBot Using Natural Language Processing in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a chatbot ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A chatbot is a computer program that is designed to simulate conversation with human users, typically over messaging platforms, such as Facebook Messenger, WhatsApp, or Slack. Chatbots use Natural Language Processing (NLP) and Artificial Intelligence (AI) to understand and respond to user queries in a human-like manner.\n",
    "\n",
    "##### Chatbots can be designed to perform a variety of tasks, such as answering customer queries, providing product recommendations, scheduling appointments, or even just engaging in casual conversation. Some chatbots are rule-based, meaning they respond based on pre-programmed rules, while others are more advanced and use machine learning algorithms to learn from user interactions and improve their responses over time.\n",
    "\n",
    "##### Chatbots have become increasingly popular in recent years, as they provide a cost-effective and scalable way for businesses to provide customer support and interact with users. They can be accessed 24/7, and can handle a high volume of inquiries without the need for human intervention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the major fields of Natural Language Processing (NLP):\n",
    "\n",
    "- Morphology: This field deals with the study of the internal structure of words and how they are formed.\n",
    "\n",
    "- Syntax: This field deals with the study of the structure of sentences and the rules governing the arrangement of words and phrases.\n",
    "\n",
    "- Semantics: This field deals with the study of meaning in language and how words and sentences are used to convey meaning.\n",
    "\n",
    "- Discourse Analysis: This field deals with the study of how language is used in a given context, including the relationship between speakers, the intended audience, and the purpose of communication.\n",
    "\n",
    "- Text Mining: This field involves the use of statistical and machine learning techniques to analyze and extract information from large collections of text.\n",
    "\n",
    "- Sentiment Analysis: This field involves the use of NLP techniques to automatically determine the sentiment or emotion expressed in a piece of text.\n",
    "\n",
    "- Named Entity Recognition: This field involves the use of NLP techniques to automatically identify and classify named entities such as people, organizations, and locations in text.\n",
    "\n",
    "- Machine Translation: This field involves the use of NLP techniques to automatically translate text from one language to another.\n",
    "\n",
    "- Speech Recognition: This field involves the use of NLP techniques to automatically transcribe spoken language into text.\n",
    "\n",
    "- Text-to-Speech: This field involves the use of NLP techniques to convert text into spoken language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note : \n",
    "\n",
    "- In Natural Language Processing (NLP), we often work with JSON data because JSON is a lightweight and flexible data format that is easy to read and write for both humans and machines. JSON stands for \"JavaScript Object Notation\" and it is a text-based format for representing data in the form of key-value pairs.\n",
    "\n",
    "- JSON is widely used in web development and API (Application Programming Interface) design, and many NLP tools and platforms also use JSON to represent and exchange data. For example, in NLP, we may use JSON to represent text data, such as sentences or documents, along with metadata such as author, date, or source.\n",
    "\n",
    "- JSON data can be easily parsed and manipulated by programming languages such as Python, making it a popular choice for NLP tasks that involve data preprocessing, analysis, or modeling. JSON also allows for hierarchical and nested data structures, which can be useful for representing complex linguistic data such as parse trees or dependency graphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Importing libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON: It is possible to utilize it to work with JSON data.\n",
    "\n",
    "- String: Provides access to several potentially valuable constants.\n",
    "\n",
    "- Random: For various distributions, this module implements pseudo-random number generators.\n",
    "\n",
    "- WordNetLemmatizer: It can lemmatize. In other terms, Lemmatization is the process of reducing a word to its base or root form. WordNetLemmatizer is available through the NLTK (Natural Language Toolkit) library\n",
    "\n",
    "- Tensorflow: A multidimensional array of elements is represented by this symbol.\n",
    "\n",
    "- Sequential: Sequential groups a linear stack of layers into a tf.keras.Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as num\n",
    "from nltk.stem import WordNetLemmatizer # It has the ability to lemmatize.\n",
    "#import tensorflow as tensorF # A multidimensional array of elements is represented by this symbol.\n",
    "#from tensorflow.keras import Sequential # Sequential groups a linear stack of layers into a tf.keras.Model\n",
    "#from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "nltk.download(\"punkt\")# required package for tokenization\n",
    "nltk.download(\"wordnet\")# word database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization is the process of breaking down a text into smaller units, called tokens. In Natural Language Processing (NLP), tokenization is often the first step in processing text data. The resulting tokens can then be used for various NLP tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "##### A token can be defined as a sequence of characters that represents a unit of meaning. The most common type of tokenization involves breaking down text into words, also known as word tokenization. However, other types of tokenization can involve breaking down text into individual characters, phrases, or sentences.\n",
    "\n",
    "##### In word tokenization, the input text is typically split into words based on whitespace or punctuation marks. For example, the sentence \"I love natural language processing!\" could be tokenized into the following words: \"I\", \"love\", \"natural\", \"language\", \"processing\". Some tokenization algorithms may also take into account additional factors such as capitalization and context.\n",
    "\n",
    "##### Tokenization can be performed using a variety of tools and libraries in NLP, such as NLTK (Natural Language Toolkit) and spaCy in Python. These tools provide pre-trained models and functions for performing tokenization, as well as options for customizing the tokenization process based on specific needs.\n",
    "\n",
    "##### Tokenization is a fundamental step in many NLP tasks, as it enables the computer to process and analyze text data at a more granular level. By breaking down text into smaller units, tokenization helps to reduce the complexity of the data and makes it easier for NLP algorithms to extract meaningful information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Loading the Dataset: intents.json\n",
    "\n",
    "data_file = open('Data.json').read()\n",
    "data = json.loads(data_file)\n",
    "\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer() #reducing words to their base or dictionary form\n",
    "\n",
    "ourClasses = []\n",
    "newWords = []\n",
    "documentX = []\n",
    "documentY = []\n",
    "# Each intent is tokenized into words and the patterns and their associated tags are added to their respective lists.\n",
    "for intent in data[\"ourIntents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        ournewTkns = nltk.word_tokenize(pattern)\n",
    "        newWords.extend(ournewTkns)\n",
    "        documentX.append(pattern)\n",
    "        documentY.append(intent['tag'])\n",
    "    if intent[\"tag\"] not in ourClasses:\n",
    "        ourClasses.append(intent[\"tag\"])\n",
    "\n",
    "newWords = [lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation]\n",
    "newWords = sorted(set(newWords))\n",
    "ourClasses = sorted(set(ourClasses))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code seems to be preparing the data that will be used to train an NLP model to recognize intents and generate appropriate responses. The newWords list is likely to be used to create a vocabulary of all the unique words that appear in the training data, while the documentX and documentY lists are probably going to be used as the input and output data for the NLP model. The ourClasses list may be used to define the set of possible intents that the chatbot can understand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
